{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all estimators list\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='classifier') # also try 'regressor' \n",
    "classification_estimators = []\n",
    "i = 1\n",
    "for name, class_ in estimators:\n",
    "    classification_estimators.append(class_.__name__)\n",
    "    print(f'{i}. {class_.__name__}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all sklearn functions\n",
    "from sklearn.utils.discovery import all_functions\n",
    "functions = all_functions()\n",
    "for name, function in functions:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning - Algorithm Tradeoffs\n",
    "\n",
    "* Linear Regression\n",
    "    * High interpretability.\n",
    "    * Lower accuracy for complex datasets.\n",
    "\n",
    "* Decision Trees\n",
    "    * Prone to overfitting, reducing accuracy.\n",
    "\n",
    "* K-Nearest Neighbours (KNN)\n",
    "    * Accuracy can be affected by irrelevant features.\n",
    "\n",
    "* Support Vector Machines (SVMs)\n",
    "    * Lower interpretability due to data transformation.\n",
    "    * Can be more accurate for certain datasets.\n",
    "\n",
    "* Random Forests\n",
    "    * Reduced interpretability due to ensemble method.\n",
    "    * Typically, higher accuracy than individual trees or KNN.\n",
    "\n",
    "* Neural Networks\n",
    "    * Least interpretable due to complex architectures.\n",
    "    * Can achieve high accuracy on complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For supervised learning:\n",
    "\n",
    "* **Categorise the problem early**\n",
    "    * Classification.\n",
    "    * Clustering.\n",
    "    * Regression.\n",
    "    * Ranking.\n",
    "\n",
    "* **Check Data Quality**\n",
    "    * Data formats and data types.\n",
    "    * Consistency of expression e.g. 5.93, $5.93, five ninety-three ...\n",
    "\n",
    "* **Reduce Data**\n",
    "    * Attribute Sampling.\n",
    "    * Record Sampling.\n",
    "    * Aggregation.\n",
    "\n",
    "* **Clean Data**\n",
    "    * Substitute or fill missing Values\n",
    "        \n",
    "* **Create New Features**\n",
    "    * E.g., Creating a ‘day’ feature from a ‘date’ feature\n",
    "\n",
    "* **Rescale Data**\n",
    "    * Data Normalisation\n",
    "        * Create a 0.0 – 1.0 range for values\n",
    "        * Encode categorical data (could be set as 0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset Demo\n",
    "\n",
    "* Exploring the `iris` dataset\n",
    "    * One of the built-in datasets in `seaborn`.\n",
    "    * Use `pandas`, `matplotlib` and `seaborn` functions to assess the data.\n",
    "        * Type and shape\n",
    "        * Feature names\n",
    "        * Target names\n",
    "        * Array types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the seaborn built-in dataset 'iris'\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "df_iris = sns.load_dataset('iris')\n",
    "\n",
    "df_iris.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We use a pairplot to see whether there is likely to be a way to separate out distinct classes in the data.\n",
    "* Grid of scatter plots.\n",
    "* Each feature plotted against every other feature.  \n",
    "\n",
    "* Reading a Pairplot\n",
    "    * Diagonal\n",
    "        * Shows distribution of a single variable.\n",
    "        * Usually a form of histogram.\n",
    "    * Off-diagonal\n",
    "        * Shows scatter plot between two variables.\n",
    "        * Rising from left to right\n",
    "            * Suggests positive correlation\n",
    "            * e.g. Petal Length and Petal Width\n",
    "\n",
    "* Insights from Pairplot:\n",
    "    * Understand relationships between variables.\n",
    "    * Identify clusters and outliers.\n",
    "    * Discover trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot\n",
    "sns.set_style(style=\"ticks\")\n",
    "sns.set_palette('viridis')\n",
    "sns.pairplot(df_iris, hue=\"species\", markers=[\"o\", \"s\", \"D\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next, a very simple classifier (a decision tree) to see whether our assumption is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# using the iris dataset again \n",
    "# we first need to encode the species names as numeric variables\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df_iris['species_num'] = label_encoder.fit_transform(df_iris['species'])\n",
    "\n",
    "# Define X and y\n",
    "X = df_iris.drop(columns = ['species','species_num'], axis = 1)\n",
    "y = df_iris['species_num']\n",
    "\n",
    "# Train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# Create instance of classifier\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Evaluate training and testing accuracy \n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "# Visualise the tree  \n",
    "from sklearn import tree \n",
    "\n",
    "# Putting the feature names and class names into variables\n",
    "fn = X.columns\n",
    "cn = df_iris['species'].unique()\n",
    "\n",
    "# making the figure larger to be more readable\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# plotting the tree\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "* Model has both a known input and a known output used for training.\n",
    "* Knows the output during the training process.\n",
    "* Trains the model to reduce the error in prediction.\n",
    "* Two major types of supervised learning methods:\n",
    "    * Classification\n",
    "    * Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "* Supervised learning method used for predicting:\n",
    "    * A continuous outcome variable (Y)\n",
    "    * Based on one or more predictor variables (X)\n",
    "* Assumes a linear relationship between the predictor variables and the outcome.\n",
    "* Coefficients in the model represent the change in the outcome associated with a one-unit change in the predictors.\n",
    "* Model fitting involves minimising the sum of the squared differences between the observed and predicted values of the outcome.\n",
    "* Performance is evaluated using measures Mean Absolute Error, Mean Squared Error and Root Mean Squared Error (RMSE).\n",
    "* Assumptions include linearity, independence, and normality of residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example with one input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the tips dataset\n",
    "df = sns.load_dataset('tips')\n",
    "print(df.sample())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df[['total_bill']]\n",
    "y = df['tip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model intercept: {}'.format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'.format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X, y, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X, linreg.coef_ * X + linreg.intercept_, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example with multiple input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the tips dataset\n",
    "df_tips = sns.load_dataset('tips')\n",
    "print(\"Dataframe before pre-processing:\")\n",
    "print(df_tips.head(1))\n",
    "\n",
    "# encode non-numeric variables\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df_tips['day'] = label_encoder.fit_transform(df_tips['day'])\n",
    "df_tips['time'] = label_encoder.fit_transform(df_tips['time'])\n",
    "df_tips['sex'] = label_encoder.fit_transform(df_tips['sex'])\n",
    "df_tips['smoker'] = label_encoder.fit_transform(df_tips['smoker'])\n",
    "\n",
    "print(\"\\nDataframe after pre-processing:\")\n",
    "print(df_tips.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y \n",
    "X = df_tips.drop(columns=['tip'])\n",
    "y = df_tips['tip']\n",
    "\n",
    "# Train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# Create another instance of the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg2 = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model intercept: {}'.format(linreg2.intercept_))\n",
    "print('linear model coeff:\\n{}'.format(linreg2.coef_))\n",
    "print('R-squared score (training): {:.3f}'.format(linreg2.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'.format(linreg2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "* A statistical model used for binary classification problems.\n",
    "* Estimates the probability of an event occurrence based on one or more predictor variables.\n",
    "* Uses logistic sigmoid function to return a probability value between 0 and 1.\n",
    "* Binary Outcome: predicts one of two possible outcomes.\n",
    "* Applications\n",
    "    * Healthcare, finance, social sciences.\n",
    "    * For risk assessment and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example using Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the tips dataset\n",
    "df_titanic = sns.load_dataset('titanic')\n",
    "df_titanic = df_titanic.dropna(how='any')\n",
    "\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df_titanic['sex']= label_encoder.fit_transform(df_titanic['sex'])\n",
    "\n",
    "# Define X and y\n",
    "X = df_titanic[['sex','age','pclass']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add log example here \n",
    "# use tips - tip Y/N\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class_names=['Perished','Survived'] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the predictor variable and the response variable\n",
    "x = df_titanic['age']\n",
    "y = df_titanic['survived']\n",
    "\n",
    "#plot logistic regression curve\n",
    "sns.regplot(x=x, y=y, data=df_titanic, logistic=True, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbours (KNN)\n",
    "\n",
    "* K-NN algorithm compares a new entry with existing data entries.\n",
    "\n",
    "* Assigns the new data a class based on closeness to neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add knn example\n",
    "# use tips tipped Y/N\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris # another way of accessing built-in datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"sepal length (cm)\", \"sepal width (cm)\"]]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meshgrid \n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['coral', 'gainsboro', 'linen'])\n",
    "cmap_bold = ListedColormap(['firebrick', 'darkslategray', 'orange'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating a model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score \n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "print(\"Accuracy \\nThe proportion of correct predictions:\", accuracy)\n",
    "print(\"\\nPrecision \\nMinimising false negatives:\", precision)\n",
    "print(\"\\nRecall \\nMinimising false positives:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalising and scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop('fraud', axis=1)\n",
    "y = df['fraud']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning  \n",
    "* Machine learning techniques for discovering patterns in data.\n",
    "\n",
    "* **Clustering**\n",
    "    * Finding natural clusters of data points based on the variables.\n",
    "\n",
    "* **Dimensionality Reduction**\n",
    "    * Searching for patterns and correlations.\n",
    "    * Using these patterns to express data in a compressed form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means Clustering**  \n",
    "* Widely used method for cluster analysis.\n",
    "* Aims to partition a set of objects into K clusters.\n",
    "* Minimises the sum of the squared distances between the objects and their assigned cluster mean.\n",
    "* Applicatons:\n",
    "    * Customer segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-means example \n",
    "Borrowed from Kaggle: https://www.kaggle.com/code/satishgunjal/tutorial-k-means-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data \n",
    "mall_df = pd.read_csv('https://raw.githubusercontent.com/jargonautical/bsuBootcampCohort5/refs/heads/main/Mall_Customers.csv')\n",
    "mall_df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does the data look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatterplot \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(mall_df['Annual Income (k$)'], mall_df['Spending Score (1-100)'])\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.title('Unlabelled Mall Customer Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "from sklearn.cluster import KMeans \n",
    "X = mall_df.iloc[:, [3,4]].values # using only the last 2 columns\n",
    "kmeans = KMeans(n_clusters=5).fit(X) # assuming 5 clusters, based on the scatterplot above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the model  \n",
    "* Unsupervised models don't have 'true positive' 'false positive' etc because nothing in the data is labelled.\n",
    "* Instead we look for the number of clusters that minimises `inertia`:   \n",
    "    (the means of the distance of each data point from the cluster centroids they have been assigned to)  \n",
    "* The elbow plot is a simple method using a `for` loop to try different numbers of clusters and plot the corresponding inertia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBOW PLOT\n",
    "from sklearn.cluster import KMeans\n",
    "X = mall_df.iloc[:, [3,4]].values \n",
    "inertias = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'random', random_state = 42).fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = kmeans.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 clusters looks like a good fit.\n",
    "* We re-fit the model specifying this, and get predictions. \n",
    "* Then we can plot the predictions and assess whether we are happy with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans= KMeans(n_clusters = 5, random_state = 42)\n",
    "# Compute k-means clustering\n",
    "kmeans.fit(X)\n",
    "# Compute cluster centers and predict cluster index for each sample.\n",
    "pred = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[pred == 0, 0], X[pred == 0, 1], c = 'deepskyblue', label = 'Cluster 0')\n",
    "plt.scatter(X[pred == 1, 0], X[pred == 1, 1], c = 'salmon', label = 'Cluster 1')\n",
    "plt.scatter(X[pred == 2, 0], X[pred == 2, 1], c = 'darkviolet', label = 'Cluster 2')\n",
    "plt.scatter(X[pred == 3, 0], X[pred == 3, 1], c = 'lawngreen', label = 'Cluster 3')\n",
    "plt.scatter(X[pred == 4, 0], X[pred == 4, 1], c = 'gold', label = 'Cluster 4')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:, 1],s = 300, c = 'firebrick', label = 'Centroid', marker='*')\n",
    "\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.legend()\n",
    "plt.title('Customer Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import the data\n",
    "df_iris = sns.load_dataset('iris')\n",
    "features = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n",
    "\n",
    "# encode species names \n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df_iris['species'] = label_encoder.fit_transform(df_iris['species'])\n",
    "\n",
    "# create an instance of the Principal Component Analysis (PCA) model \n",
    "pca = PCA(n_components=2) # we specify that we are reducing the number of components to 2\n",
    "components = pca.fit_transform(df[features]) # then we fit this instance to the features in the dataframe\n",
    "\n",
    "# plot the results \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(components[:, 0], # position on the first principal component of the observations\n",
    "            components[:, 1], alpha=0.7, c=df_iris['species'], cmap='viridis') # position on the second principal component of the observations\n",
    "            \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
