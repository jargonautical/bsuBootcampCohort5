{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling and mining with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping data  \n",
    "\n",
    "* Uses the `groupby` function.  \n",
    "* Reshapes (groups) the data and applies an aggregrate function e.g. `sum`, `mean`, `count`.\n",
    "* A powerful tool for exploratory data analysis.  \n",
    "* Functionality similar to Excel's pivot tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Category': ['Electronics', 'Clothing', 'Electronics', 'Clothing'],\n",
    "        'Sales': [1000, 500, 800, 500]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Grouping by 'Category'\n",
    "grouped_data = df.groupby('Category')\n",
    "\n",
    "# Choosing sales column to compare with grouped data\n",
    "# Using sum function\n",
    "# Gives the total sales for each category\n",
    "total_sales = grouped_data['Sales'].sum()\n",
    "total_sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by multiple columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Class': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "        'Gender': ['Male', 'Male', 'Female', 'Female', 'Male', 'Female'],\n",
    "        'Math_Score': [85, 92, 78, 89, 90, 86],\n",
    "        'English_Score': [88, 94, 80, 92, 92, 88]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Class' and 'Gender' and calculating statistics\n",
    "grouped_data = df.groupby(['Class', 'Gender'])\n",
    "\n",
    "# Calculate the mean for Math_score\n",
    "agg_results = grouped_data['Math_Score'].mean()\n",
    "\n",
    "print(agg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__ If we don;t specify which column to calculate the aggregate values for, pandas will apply the calculation to __all__ numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Class': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "        'Gender': ['Male', 'Male', 'Female', 'Female', 'Male', 'Female'],\n",
    "        'Math_Score': [85, 92, 78, 89, 90, 86],\n",
    "        'English_Score': [88, 94, 80, 92, 92, 88]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Grouping by 'Class' and 'Gender'\n",
    "grouped_data = df.groupby(['Class', 'Gender'])\n",
    "\n",
    "# Applying the mean aggregation function to all numeric columns\n",
    "aggregated_data = grouped_data.mean()\n",
    "\n",
    "print(aggregated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Group By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Given a DataFrame ‘df’ with columns ‘A’, ‘B’, and ‘C’, where ‘A’ and ‘B’ are categorical variables and ‘C’ is a numerical variable, write a code to group the DataFrame by column ‘A’ and calculate the sum of ‘C’ for each group.\n",
    "\n",
    "```\n",
    "'Store': ['Store1', 'Store2', 'Store1', 'Store2', 'Store1', 'Store2', 'Store1', 'Store1'],\n",
    "'Product': ['Apple', 'Banana', 'Cherry', 'Apple', 'Banana', 'Cherry', 'Apple', 'Banana'],\n",
    "'Sales': [100, 200, 150, 300, 250, 150, 200, 300]\n",
    "```\n",
    "\n",
    "Note: The groupby function is used to split the data into groups based on some criteria. The sum function is then used to calculate the sum of ‘C’ for each group.\n",
    "\n",
    "```\n",
    "Do not forget to import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Now, modify your code to group the DataFrame by both columns ‘A’ and ‘B’, and calculate the sum of ‘C’ for each group.\n",
    "\n",
    "```\n",
    "Note: You can pass a list of column names to the groupby function to group by multiple columns. The resulting groups are hierarchical and can provide more detailed insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Given a DataFrame ‘df’ with columns ‘City’, ‘Property_Type’, and ‘Price’, where ‘City’ and ‘Property_Type’ are categorical variables and ‘Price’ is a numerical variable, write a code to group the DataFrame by column ‘City’ and calculate the total price for each city.\n",
    "\n",
    "```\n",
    "'City': ['London', 'Paris', 'London', 'Paris', 'London', 'Paris', 'London', 'London'],\n",
    "'Property_Type': ['House', 'Apartment', 'House', 'Apartment', 'House', 'Apartment', 'House', 'Apartment'],\n",
    "'Price': [500000, 400000, 600000, 450000, 550000, 500000, 650000, 700000]\n",
    "```\n",
    "\n",
    "Note: The groupby function is used to split the data into groups based on some criteria. The sum function is then used to calculate the sum of ‘Price’ for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Now, modify your code to group the DataFrame by both columns ‘City’ and ‘Property_Type’, and calculate the total price for each combination.\n",
    "\n",
    "Note: You can pass a list of column names to the groupby function to group by multiple columns. The resulting groups are hierarchical and can provide more detailed insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions  \n",
    "\n",
    "* Essential for summarising data  \n",
    "* Applied within groups for meaningful insights  \n",
    "* Common functions are:  \n",
    "    * `sum()` - total of values  \n",
    "    * `max()` - maximum value in the column  \n",
    "    * `min()` - smallest value in the column  \n",
    "    * `mean()` - mean average of the values  \n",
    "    * `median()` - median of the values  \n",
    "    * `count()` - number of values in the column  \n",
    "* Custom aggregation functions:  \n",
    "    * `agg()` - allows application of custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Class': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "        'Gender': ['Male', 'Male', 'Female', 'Female', 'Male', 'Female'],\n",
    "        'Math_Score': [85, 92, 78, 89, 90, 86],\n",
    "        'English_Score': [88, 94, 80, 92, 92, 88],\n",
    "        'Physics_Score': [78, 90, 85, 92, 88, 84]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Grouping by 'Class' and 'Gender' and calculating statistics\n",
    "grouped_data = df.groupby(['Class', 'Gender'])\n",
    "\n",
    "# Calculate the mean, min, and max scores for Math_score\n",
    "agg_results = grouped_data.Math_Score.agg(['mean', 'min', 'max'])\n",
    "\n",
    "print(agg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying aggregation functions to 'Math_Score' and 'Physics_Score'\n",
    "aggregated_data = grouped_data.agg({\n",
    "    'Math_Score': ['mean', 'min', 'max'],\n",
    "    'Physics_Score': ['mean', 'min', 'max']\n",
    "})\n",
    "\n",
    "print(aggregated_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables and cross-tabulation  \n",
    "\n",
    "* \n",
    "* Efficent tools for data analysis.  \n",
    "* Provide structured ways to arrange and analyse data.  \n",
    "\n",
    "#### Pivot table  \n",
    "* Uses the `pd.pivot_table()` function.  \n",
    "* Data is categorised into a two-dimensional table, used to:  \n",
    "    * Summarise,\n",
    "    * Analyse,  \n",
    "    * Explore, \n",
    "    * Present.  \n",
    "\n",
    "#### Cross-tabulations (crosstabs)  \n",
    "* Uses the `pd.crosstab()` function.\n",
    "* Aggregates data.\n",
    "* Mainly used for categorical variables.\n",
    "* Quantifies the relationship between 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with sales data\n",
    "data = {'Category': ['Electronics', 'Clothing', 'Electronics', 'Clothing'],\n",
    "        'Region': ['North', 'South', 'North', 'South'],\n",
    "        'Sales': [1000, 500, 800, 750],\n",
    "        'Profit': [150, 50, 120, 100]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot Table: Sum of Sales by Category and Region\n",
    "pivot_table = pd.pivot_table(df, index='Category', \n",
    "                             columns='Region', \n",
    "                             values='Sales', \n",
    "                             aggfunc='sum')\n",
    "\n",
    "print(\"Pivot Table:\", pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Tabulation: Count of Category by Region\n",
    "cross_tab = pd.crosstab(df['Category'], df['Region'])\n",
    "\n",
    "print(\"\\nCross-Tabulation:\", cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Data aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Given the same DataFrame, write a code to calculate the sum, max, min, mean, median and count of column ‘C’.\n",
    "\n",
    "```\n",
    "'Store': ['Store1', 'Store2', 'Store1', 'Store2', 'Store1', 'Store2', 'Store1', 'Store1'],\n",
    "'Product': ['Apple', 'Banana', 'Cherry', 'Apple', 'Banana', 'Cherry', 'Apple', 'Banana'],\n",
    "'Sales': [100, 200, 150, 300, 250, 150, 200, 300]\n",
    "```\n",
    "Note: The groupby function is used to split the data into groups based on some criteria. The sum function is then used to calculate the sum of ‘Sales’ for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Now, modify your code to calculate these aggregation functions for each group of column ‘Store’.\n",
    "\n",
    "Note: You can combine groupby and agg functions to apply multiple aggregations on each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Given the same DataFrame, write a code to calculate the total (sum), maximum (max), minimum (min), average (mean), middle value (median) and number (count) of prices.\n",
    "\n",
    "Note: The agg function is used to apply one or more operations over specified axis. It’s a flexible way to apply multiple aggregations on a Series or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Now, modify your code to calculate these aggregation functions for each group of column ‘City’.\n",
    "\n",
    "Note: You can combine groupby and agg functions to apply multiple aggregations on each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Write a code to calculate the range (max - min) of prices for each city. You need to define a custom function for calculating range and use it in agg().\n",
    "\n",
    "Note: The agg function also allows you to use custom functions. This provides great flexibility in performing complex aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualisation  \n",
    "\n",
    "* Powerful tool for exploring data and for communicating insights.  \n",
    "* Helps us to understand complex data patterns.  \n",
    "* Finding correlation:\n",
    "    * Identify relationships between variables.  \n",
    "    * Predict trends and help to make decisions.  \n",
    "* Sorting data:  \n",
    "    * Organising data in a meaningful order.  \n",
    "    * Display rather than analysis tool, aids interpretation.  \n",
    "* Creating basic plots:  \n",
    "    * Representing the data visually.\n",
    "    * Helps to identify patterns, trends, and outliers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations  \n",
    "\n",
    "* Uses the `df.corr()` function.  \n",
    "* Applies to a whole dataframe.  \n",
    "* Shows the correlation of every column to every other column.  \n",
    "\n",
    "* Finding correlation:  \n",
    "    * Quantifies the relationshop between variables.  \n",
    "    * Shows __how__ variables interact with each other.  \n",
    "* Understanding correlation:  \n",
    "    * Values closer to +1 indicate a __strong positive correlation__.  \n",
    "    * Values closer to -1 indicate a __strong negative correlation__.  \n",
    "    * Values closer to 0 indicate __no correlation__ - the variables are not related.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install to environment\n",
    "#    on first run\n",
    "# %pip install matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'A': [1, 2, 3, 4, 5],\n",
    "        'B': [5, 4, 3, 2, 1],\n",
    "        'C': [2, 3, 5, 7, 11]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Finding Correlation\n",
    "print(\"Correlation:\")\n",
    "print(df.corr())\n",
    "\n",
    "# Sorting Data\n",
    "print(\"\\nSorted Data:\")\n",
    "print(df.sort_values('B'))\n",
    "\n",
    "# Creating Basic Plot\n",
    "df.plot(x='A',y='B',kind='scatter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Category': ['Electronics', 'Clothing', 'Electronics', 'Books', 'Homewares'],\n",
    "        'Region': ['North', 'South', 'North', 'South', 'West'],\n",
    "        'Sales': [1000, 500, 800, 500, 700],\n",
    "        'Profit': [150, 50, 120, 120,180]}\n",
    "dfx = pd.DataFrame(data)\n",
    "\n",
    "dfx['Sales'].plot(kind='hist', bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The range of chart options  \n",
    "\n",
    "* Line plot\n",
    "* Bar plot\n",
    "* Horizontal bar plot \n",
    "* Histogram  \n",
    "* Box plot  \n",
    "* Area plot  \n",
    "* Scatter plot  \n",
    "* Pie chart  \n",
    "* Hexbin plot  \n",
    "* KDE plot  \n",
    "* Density plot  \n",
    "* Boxen plot  \n",
    "\n",
    "The goal is to select the plot that best represents your data and makes it easier to understand.  \n",
    "Each type of plot is suited to a specific type of data, data structure, and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Constructing a basic line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Plot\n",
    "# Ideal for showing trends over time\n",
    "# Example: Stock prices over a period\n",
    "data = {'Date': pd.date_range(start='1/1/2020', periods=5),\n",
    "        'Stock_Price': [1, 2, 4, 8, 16]}\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(x='Date', y='Stock_Price', kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different types of plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Plot\n",
    "df.plot(x='X', y='Y', kind='line')\n",
    "#Bar Plot\n",
    "df.plot(x='Category', y='Count', kind='bar')\n",
    "# Horizontal Bar Plot\n",
    "df.plot(x='Count', y='Category', kind='barh')\n",
    "# Histogram\n",
    "df['Value'].plot(kind='hist', bins=20)\n",
    "# Box Plot\n",
    "df.plot(y='Value', kind='box')\n",
    "# Area Plot\n",
    "df.plot(x='X', y='Y', kind='area')\n",
    "# Scatter Plot\n",
    "df.plot(x='X', y='Y', kind='scatter')\n",
    "# Pie Chart\n",
    "df['Category'].value_counts().plot(kind='pie')\n",
    "# Hexbin Plot\n",
    "df.plot(x='X', y='Y', kind='hexbin', gridsize=20)\n",
    "# Stacked Bar Plot\n",
    "df.pivot_table(index='Category', \n",
    "               columns='Subcategory', \n",
    "               values='Value', \n",
    "               aggfunc='sum').plot(kind='bar', stacked=True)\n",
    "# Line plot with multiple Lines\n",
    "df.plot(x='Date',y=['Series1','Series2'],kind='line')\n",
    "            \n",
    "#Advanced Plots\n",
    "# KDE Plot (Kernel Density Estimate)\n",
    "df['Value'].plot(kind='kde')\n",
    "# Density Plot\n",
    "df['Value'].plot(kind='density')\n",
    "# Boxen Plot\n",
    "df.plot(y='Value', kind='boxen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot\n",
    "# Useful for comparing quantities of different categories\n",
    "# Example: Sales data by product category\n",
    "data = {'Category': ['A', 'B', 'C'],\n",
    "        'Sales': [1000, 2000, 1500]}\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(x='Category', y='Sales', kind='bar')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal Bar Plot\n",
    "# Same as bar plot but categories are on y-axis\n",
    "# Example: Population of countries\n",
    "data = {'Country': ['Country A', 'Country B', 'Country C'],\n",
    "        'Population': [1000000, 2000000, 1500000]}\n",
    "df = pd.DataFrame(data)\n",
    "# Ensure 'Population' is numeric\n",
    "df['Population'] = pd.to_numeric(df['Population'])\n",
    "df.plot(y='Population', x='Country', kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz = pd.read_csv('counters_data.csv')\n",
    "dfz.head()\n",
    "dfz[' DVPL1'].plot(kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "# Perfect for visualizing distribution of numerical data\n",
    "# Example: Grades of students in a class\n",
    "data = {'Grades': [85, 90, 67, 92, 88]}\n",
    "df = pd.DataFrame(data)\n",
    "df['Grades'].plot(kind='hist', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "# Shows the quartiles of dataset and identifies outliers\n",
    "# Example: Exam scores of students\n",
    "data = {'Scores': [85, 90, 78, 92, 88]}\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Plot\n",
    "# Good for comparing two or more quantities\n",
    "# Example: Stock volume over time\n",
    "data = {'Date': pd.date_range(start='1/1/2020', periods=5),\n",
    "        'Stock_Volume': [1000, 2000, 1500, 1800, 1200]}\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(x='Date', y='Stock_Volume', kind='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot\n",
    "# Great for showing the relationship between two variables\n",
    "# Example: Relationship between age and income\n",
    "data = {'Age': [25, 30, 35, 40, 45],\n",
    "        'Income': [22000, 26000, 31000, 70000, 87000]}\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(x='Age', y='Income', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart\n",
    "# Suitable for displaying proportion of categories in a whole\n",
    "# Example: Market share of companies\n",
    "data = {'Company': ['Company A', 'Company B', 'Company C'],\n",
    "        'Market_Share': [65, 224, 20]}\n",
    "df = pd.DataFrame(data)\n",
    "df['Market_Share'].plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Given a small DataFrame ‘df_small’ with columns ‘Age’, ‘Income’, and ‘Spending_Score’, where all are numerical variables, write a code to calculate the correlation between these variables.\n",
    "\n",
    "```\n",
    "'Age': [25, 35, 45, 55, 65],\n",
    "'Income': [30000, 40000, 50000, 60000, 70000],\n",
    "'Spending_Score': [20, 40, 60, 80, 100]\n",
    "```\n",
    "\n",
    "Note: The corr function is used to compute pairwise correlation of columns. Correlation coefficients quantify the degree to which a relationship between two variables can be described by a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Given a DataFrame ‘df_bar’ with columns ‘Product’ and ‘Sales’, where ‘Product’ is a categorical variable and ‘Sales’ is a numerical variable, write a code to create a bar plot showing the total sales for each product.\n",
    "\n",
    "```\n",
    "'Product': ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry'],\n",
    "'Sales': [1000, 2000, 1500, 1800, 1200]\n",
    "```\n",
    "\n",
    "Note: The plot function with kind=‘bar’ is used to create bar plots. Bar plots are useful for comparing quantities of different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Create a DataFrame with columns ‘Age’ and ‘Income’, where both are numerical variables, write a code to create a scatter plot showing the relationship between age and income.\n",
    "\n",
    "```\n",
    "    Remember data must be numeric type\n",
    "\n",
    "\n",
    "    # import random\n",
    "    import random\n",
    "    \n",
    "    'Age': [random.randint(20, 70) for _ in range(100)],\n",
    "    'Income': [random.randint(20000, 70000) for _ in range(100)]\n",
    "```\n",
    "\n",
    "Note: Scatter plots are useful for visualizing relationships between two numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Given a DataFrame ‘df_hist’ with column ‘Scores’, which is a numerical variable representing scores of students in an exam, write a code to create a histogram showing the distribution of scores.\n",
    "\n",
    "```\n",
    "'Scores': [80, 85, 90, 95, 100, 97, 100]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data\n",
    "\n",
    "This is data collected or recorded at specific, __meaningful__ time intervals.  \n",
    "* Stock prices at end of trading each day  \n",
    "* Temperature variations hourly  \n",
    "* Sales by day, for significant calendar events and celebrations  \n",
    "\n",
    "A time series helps us analyse trends, patterns and cycles over time.  \n",
    "This type of data requires special methods and tools for processing and visualising.  \n",
    "\n",
    "Pandas can handle various formats and operations on date/time data, and has functionality for plotting it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Date': pd.date_range(start='1/1/2022', periods=5),\n",
    "        'Stock_Price': [150, 152, 154, 153, 155]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the data\n",
    "df.plot(x='Date', y='Stock_Price', kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datetime functions and tools  \n",
    "\n",
    "Uses `pd.to_datetime()` function  \n",
    "\n",
    "`datetime` has a specific meaning in Python and in Pandas, and there are similar functions in other programming languages.  \n",
    "The tools in Pandas allow us to convert a series (column) with date/time values into a pandas `datetime` series. The values are standardised and can be desconstructed to year, month, day, hours, minutes and seconds (if those exist in your values).  \n",
    "\n",
    "Examples of where it might be used:\n",
    "* Analysing sales data to identify trends in buying based on the time of purchase.\n",
    "* Studying website traffic to understand user behaviour during different times of the day, or on different days of the week.  \n",
    "* Examining weather data to observe seasonal variations, or to monitor conditions and impacts in agriculture or tourism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sales_data' has a DataFrame with a 'Purchase_Time' column\n",
    "sales_data = pd.DataFrame({'Purchase_Time': ['2023-01-01 08:30:00', \n",
    "                                             '2023-02-01 14:45:00', \n",
    "                                             '2023-03-01 08:15:00']})\n",
    "\n",
    "# the Purchase_Time column is converted to a datetime column \n",
    "sales_data['Purchase_Time'] = pd.to_datetime(sales_data['Purchase_Time'])\n",
    "\n",
    "# year, month, day, hour etc can be extracted using dt.year(), dt.month(), dt.day(), dt.hour()\n",
    "sales_data['Purchase_Hour'] = sales_data['Purchase_Time'].dt.hour\n",
    "\n",
    "# Now you can analyse sales trends based on the hour of purchase\n",
    "hourly_sales = sales_data.groupby('Purchase_Hour').size()\n",
    "\n",
    "print(hourly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'traffic_data' is a DataFrame with a 'Visit_Time' column\n",
    "traffic_data = pd.DataFrame({'Visit_Time': ['2023-01-01 08:30:00', \n",
    "                                            '2023-02-01 14:45:00', \n",
    "                                            '2023-03-05 20:15:00']})\n",
    "traffic_data['Visit_Time'] = pd.to_datetime(traffic_data['Visit_Time'])\n",
    "traffic_data['Visit_Hour'] = traffic_data['Visit_Time'].dt.hour\n",
    "traffic_data['Visit_Day'] = traffic_data['Visit_Time'].dt.day\n",
    "\n",
    "traffic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can analyze user behavior based on the time and day of visit\n",
    "hourly_visits = traffic_data.groupby('Visit_Hour').size()\n",
    "daily_visits = traffic_data.groupby('Visit_Day').size()\n",
    "\n",
    "print(\"Hourly Visits\")\n",
    "hourly_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDaily Visits\")\n",
    "daily_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with a DateTime column\n",
    "data = {'DateTime': ['2023-01-01 08:30:00', \n",
    "                     '2023-02-01 14:45:00', \n",
    "                     '2023-03-01 20:15:00']}\n",
    "dfz = pd.DataFrame(data)\n",
    "dfz['DateTime'] = pd.to_datetime(dfz['DateTime'])\n",
    "\n",
    "# Convert the 'DateTime' column to DateTime\n",
    "#df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "\n",
    "# Extract year, month, day, and hour\n",
    "dfz['Year'] = dfz['DateTime'].dt.year\n",
    "dfz['Month'] = dfz['DateTime'].dt.month\n",
    "dfz['Day'] = dfz['DateTime'].dt.day\n",
    "dfz['Hour'] = dfz['DateTime'].dt.hour\n",
    "\n",
    "dfz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling  \n",
    "\n",
    "* A method for changing the frequency of time series data.  \n",
    "* Aggregates or transforms data from one frequency to another, e.g. from daily to monthly.  \n",
    "\n",
    "Why resample?  \n",
    "* Aggregation  \n",
    "    * Useful for managing high-frequency data e.g. minute-by-minute web traffic logs.\n",
    "    * Provides a broader overview.  \n",
    "    * Makes the data esier to work with.\n",
    "* Interpolation  \n",
    "    * Helpful when dealing with data captured at irregular intervals.  \n",
    "    * Resample it to a regular frequency for easier analysis or more effective visualisation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with daily sales data\n",
    "data = {'Date': pd.date_range(start='2023-01-01', periods=40, freq='D'),\n",
    "        'Sales': [i for i in range(40)]}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n",
    "\n",
    "# Resample data to monthly frequency, calculating the sum of sales\n",
    "monthly_sales = df.resample('M', on='Date').sum()\n",
    "\n",
    "print(monthly_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range from '2023-01-01' for 365 days with daily frequency\n",
    "date_range = pd.date_range(start='2023-01-01', periods=365, freq='D')\n",
    "\n",
    "# Create sales data for each day\n",
    "# Here we're just using numbers from 1 to 365 for simplicity\n",
    "sales_data = [i for i in range(1, 366)]\n",
    "\n",
    "# Create a DataFrame with 'Date' and 'Sales' columns\n",
    "data = {'Date': date_range, 'Sales': sales_data}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify its structure\n",
    "print('Check the dataframe structure:\\n')\n",
    "print(df.head())\n",
    "\n",
    "# Resample the data to monthly frequency\n",
    "# The 'M' argument in the resample function stands for 'Month end frequency'\n",
    "# The 'on' argument specifies the column to resample on,\n",
    "#   which is 'Date' in this case\n",
    "monthly_sales = df.resample('M', on='Date')\n",
    "\n",
    "# Calculate the sum of sales for each month using the sum function\n",
    "# This gives us the total sales for each month\n",
    "monthly_sales_sum = monthly_sales.sum()\n",
    "\n",
    "# Print the monthly sales data\n",
    "print('\\nCheck the resampled data:')\n",
    "monthly_sales_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting  \n",
    "\n",
    "Another resampling method; this moves data points forward or backward in time.  \n",
    "Used to calculate differences or time-based features in time series data.  \n",
    "\n",
    "Use cases:  \n",
    "* Calculating differences  \n",
    "    * Understanding trends or changes in the data by comparing each data point with the previous day.\n",
    "* Time lags  \n",
    "    * Understand how past values of a variable affetc future outcomes.\n",
    "\n",
    "Examples:\n",
    "* Calculate one-day price changes in a particular stock, by subtracting the previous day's price from the current day's price. \n",
    "* Can help identify price fluctuations and volatility in the stock market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with daily stock prices\n",
    "data = {'Date': pd.date_range(start='2023-01-01', \n",
    "                              periods=5, \n",
    "                              freq='D'),\n",
    "        'Price': [100, 105, 110, 108, 112]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate one-day price changes (time lag of 1 day)\n",
    "df['Price_Change'] = df['Price'] - df['Price'].shift(1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Statistics  \n",
    "\n",
    "Calculations applied to a fixed-size window of data points in a time series.  \n",
    "* The window moves through the dataset one step at a time.  \n",
    "* Also known as 'window functions' (e.g. in Spark).  \n",
    "\n",
    "Purpose:  \n",
    "* To smooth out noise in the data.\n",
    "* Makes underlying patterns more visible.  \n",
    "* Helps detect trends or patterns over time, such a moving average.  \n",
    "\n",
    "Components:  \n",
    "* Window size  \n",
    "    * The window has a fixed size specified by you.\n",
    "    * That size determines how many data points are considered.  \n",
    "* Rolling function  \n",
    "    * The function you want applied to the data within the rolling window.  \n",
    "    * Common choices are mean, sum, standard deviation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with daily stock prices\n",
    "data = {'Date': pd.date_range(start='2023-01-01', periods=10, freq='D'),\n",
    "        'Price': [100, 105, 110, 108, 112, 115, 118, 120, 122, 125]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the 3-day rolling mean (moving average) of prices\n",
    "df['Rolling_Mean'] = df['Price'].rolling(window=3).mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'weather_data' is a DataFrame with a 'Date' and 'Temperature' columns\n",
    "weather_data = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),\n",
    "    'Temperature': [20 + i*0.01 for i in range(365)]\n",
    "})\n",
    "\n",
    "# Set 'Date' as the index\n",
    "weather_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Calculate the 30-day moving average temperature\n",
    "weather_data['30_day_MA'] = weather_data['Temperature'].rolling(window=5).mean()\n",
    "\n",
    "weather_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Given a DataFrame ‘df_date’ with a column ‘Date’ containing dates in the format ‘YYYY-MM-DD’, write a code to convert the ‘Date’ column to datetime format.\n",
    "\n",
    "```\n",
    "'Date': ['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01', '2023-05-01', '2023-06-01'],\n",
    "'Value': [1, 2, 3, 4, 5, 6]\n",
    "```\n",
    "\n",
    "Note: The pd.to_datetime() function is used to convert the ‘Date’ column to datetime format. This is useful when you want to perform time series analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Now that you have converted the ‘Date’ column to datetime format, write a code to set this column as the index of the DataFrame.\n",
    "\n",
    "\n",
    "Note: Setting the ‘Date’ column as the index allows you to easily perform operations on specific dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Write a code to resample the DataFrame at a monthly frequency and calculate the mean of the ‘Value’ column for each month.\n",
    "\n",
    "Note: The resample() function is used to resample time-series data. The string ‘M’ is used for monthly frequency. Then use mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Given a DataFrame ‘df1’ with columns ‘Date’, ‘Temperature’, ‘Humidity’, ‘WindSpeed’, and ‘Rainfall’, where ‘Date’ is in the format ‘YYYY-MM-DD’ and the rest are numerical variables, write a code to resample the DataFrame at a monthly frequency and calculate the mean of all columns for each month.\n",
    "\n",
    "```\n",
    "'Date': pd.date_range(start='2023-01-01', periods=6),\n",
    "'Temperature': [20, 21, 19, 22, 20, 21],\n",
    "'Humidity': [30, 32, 31, 29, 30, 31],\n",
    "'WindSpeed': [10, 11, 10, 12, 11, 10],\n",
    "'Rainfall': [0, 0.2, 0.1, 0.3, 0.2, 0]\n",
    "```\n",
    "\n",
    "Note: The resample() function is used to resample time-series data. The string ‘M’ is used for monthly frequency. The mean() function is then used to calculate the mean of all columns for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Given a DataFrame ‘df1’ with columns ‘Date’, ‘Temperature’, ‘Humidity’, ‘WindSpeed’, and ‘Rainfall’, where ‘Date’ is in the format ‘YYYY-MM-DD’ and the rest are numerical variables, write a code to calculate the rolling mean of ‘Temperature’ with a window size of 3.\n",
    "\n",
    "```\n",
    "'Date': pd.date_range(start='2023-01-01', periods=6),\n",
    "'Temperature': [20, 21, 19, 22, 20, 21],\n",
    "'Humidity': [30, 32, 31, 29, 30, 31],\n",
    "'WindSpeed': [10, 11, 10, 12, 11, 10],\n",
    "'Rainfall': [0, 0.2, 0.1, 0.3, 0.2, 0]\n",
    "```\n",
    "\n",
    "Note: The rolling() function is used to calculate the rolling mean of a time series. The window size determines the number of observations used for calculating the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Now consider another DataFrame ‘df2’ with the same columns but different data. Modify your code to calculate the rolling standard deviation of ‘Humidity’ with a window size of 2.\n",
    "\n",
    "function = .std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical data\n",
    "\n",
    "This is any data that can be grouped into categories or labels, and will need special treatment to be used efeectively in machine learning models.  \n",
    "\n",
    "Types of categorical data:  \n",
    "* Nominal  \n",
    "    * Data with no inherent order or ranking.  \n",
    "    * E.g. colours, names, genders.  \n",
    "* Ordinal  \n",
    "    * Data with a clear order or ranking.\n",
    "    * E.g. Ratings, sizes, grades.\n",
    "\n",
    "Challenges with categorical data:  \n",
    "* Not numeric, so it cannot be used directly in mathematical operations or calculations.  \n",
    "* May have high cardinality.\n",
    "    * Can have many unique values or categories.\n",
    "    * Can cause memory and performance issues.  \n",
    "\n",
    "How we handle categorical data:\n",
    "* Encoding\n",
    "    * Converting categorical data into numeric values so it can be used in machine learning models.\n",
    "        * Label encoding  \n",
    "            * Assigning a unique number to each category (useful for ordinal data).\n",
    "        * One-hot encoding \n",
    "            * Creating a binary column for each category (useful for nominal data).\n",
    "* Feature selection\n",
    "    * Reducing the number of categories or features to avoid overfitting or complexity.\n",
    "        * Frequency thresholding \n",
    "            * Removing the categories that have low frequency or occurence in the data.\n",
    "        * Feature importance  \n",
    "            * Selecting categories that have high importance or impact on the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding  \n",
    "\n",
    "Uses `pd.get_dummies()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with a categorical column\n",
    "data = {'Category': ['A', 'B', 'A', 'C', 'B'],\n",
    "         'Count':[1,2,3,4,5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "encoded_df = pd.get_dummies(df, columns=['Category'])\n",
    "encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting ordinal data  \n",
    "\n",
    "Uses `pd.Categorical()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with an ordinal column\n",
    "data = {'Product': ['Product A', \n",
    "                    'Product B', \n",
    "                    'Product C', \n",
    "                    'Product D'],\n",
    "        'Size': ['Medium', 'Small', 'Large', 'Medium']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the custom ordinal order\n",
    "ordinal_order = ['Small', 'Medium', 'Large']\n",
    "\n",
    "# Before Sorting\n",
    "print('Normal Sorting:')\n",
    "df.sort_values(by='Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame based on the 'Size' column\n",
    "df['Size'] = pd.Categorical(df['Size'], \n",
    "                            categories=ordinal_order, \n",
    "                            ordered=True)\n",
    "print('Ordinal Sorting:')\n",
    "df.sort_values(by='Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of shirt sizes\n",
    "df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "                   'Size': ['large', 'small', 'extra large', 'medium', 'small']})\n",
    "\n",
    "# Convert the \"Size\" column to a pandas categorical column with the specified order\n",
    "ordinal_order = ['small', 'medium', 'large', 'extra large']\n",
    "df['Size'] = pd.Categorical(df['Size'], categories=ordinal_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by the \"Size\" column\n",
    "df = df.sort_values(by='Size')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Given a DataFrame ‘df1’ with a column ‘Size’ containing ordinal data (‘Small’, ‘Medium’, ‘Large’), write a code to sort the DataFrame based on the ‘Size’ column in ascending order.\n",
    "\n",
    "```\n",
    "'Product': ['Product A', 'Product B', 'Product C', 'Product D', 'Product E'],\n",
    "'Size': ['Medium', 'Small', 'Large', 'Small', 'Medium']\n",
    "```\n",
    "\n",
    "Note: The pd.Categorical() function is used to convert the ‘Size’ column to a categorical type with ordered categories. The sort_values() function is then used to sort the DataFrame by the ‘Size’ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Now consider another DataFrame ‘df2’ with the same columns but different data. Modify your code to sort this DataFrame based on the ‘Size’ column in descending order.\n",
    "\n",
    "```\n",
    "'Product': ['Product F', 'Product G', 'Product H', 'Product I', 'Product J'],\n",
    "'Size': ['Large', 'Medium', 'Small', 'Large', 'Medium']\n",
    "```\n",
    "\n",
    "Note: The sort_values() function with ascending=False is used to sort the DataFrame by the ‘Size’ column in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Given a DataFrame ‘df3’ with a column ‘Color’ containing nominal data (‘Red’, ‘Blue’, and ‘Green’), write a code to convert this column into dummy variables.\n",
    "\n",
    "```\n",
    "'Product': ['Product K', 'Product L', 'Product M'],\n",
    "'Color': ['Red', 'Blue', 'Green']\n",
    "```\n",
    "\n",
    "Note: The pd.get_dummies() function is used to convert categorical variable(s) into dummy/indicator variables. Each category becomes a new column in the DataFrame and is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Now consider another DataFrame ‘df4’ with a column ‘Age’ containing numerical data. Write a code to divide this column into bins using pd.cut() and pd.qcut().\n",
    "\n",
    "```\n",
    "\"Name\": [\"Person A\", \"Person B\", \"Person C\", \"Person D\", \"Person E\"],\n",
    "\"Age\": [20, 25, 30, 35, 40]\n",
    "```\n",
    "\n",
    "Note: The pd.cut() function is used to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. On the other hand, pd.qcut() is a quantile-based discretization function which discretize variable into equal-sized buckets based on rank or based on sample quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise operations (eval)  \n",
    "\n",
    "__Element-wise__ refers to operations that are performed on each element of a dataframe or series individually, in contrast to operations that are performed on entire rows or columns at once.  \n",
    "\n",
    "* Uses `eval()` function.\n",
    "* Allows efficient element-wise operations on a dataframe using a string expression.\n",
    "* Particularly useful for large dataframes and complex operations.\n",
    "\n",
    "Example:\n",
    "* We can use `eval()` to create a new column 'C', by performing the element-wise additions of columns 'A' and 'B'. \n",
    "* If we specify the `inplace=True` parameter, this updates the dataframe in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'A': [1,2,3,4,5],\n",
    "    'B': [2,2,2,2,2]\n",
    "})\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not element-wise \n",
    "# Using standard pandas operations\n",
    "df['C'] = df['A'] + df['B']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval() function for element-wise operations\n",
    "df.eval('C = A + B', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same! However, `eval()` can be more efficient with large dataframes and complex expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = {'A': [1, 2, 3, 4],\n",
    "        'B': [10, 20, 30, 40]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Using eval to create a new column C\n",
    "df.eval('C = A + B', inplace=True)\n",
    "\n",
    "# Calculate a new column 'D' using a complex expression\n",
    "df.eval('D = (A * 2) + (B / 3)', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering  \n",
    "\n",
    "* Uses `query()` function.\n",
    "* Similar functionality to SQL __WHERE__ clause.  \n",
    "* Filters rows in a dataframe based on a specified condition, using a string expression.\n",
    "* Useful in scenations such as demographic analysis, e.g. where you want to focus on a specific age group or income band.  \n",
    "* For complex expressions, we can use parentheses (brackets) to control order or operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Age': [25, 30, 35, 40],\n",
    "        'Salary': [50000, 60000, 70000, 80000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Using query to filter rows\n",
    "filtered_df = df.query('Age > 30')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use query to filter rows based on multiple conditions\n",
    "filtered_df = df.query('(Age > 30) and (Salary > 60000)')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `eval()` and `query()` together; particularly useful when dealing with large datasets and complex conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with employee data\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "        'Age': [25, 30, 35, 40, 45],\n",
    "        'Salary': [50000, 60000, 70000, 80000, 90000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Use eval() to calculate a new column 'Tax' \n",
    "# which is 10% of the salary\n",
    "df.eval('Tax = Salary * 0.1', inplace=True)\n",
    "\n",
    "# Use query() to filter employees who are over 30 years old\n",
    "# and have a tax greater than 5000\n",
    "filtered_df = df.query('Age > 30 and Tax > 5000')\n",
    "\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Element-wise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Given a DataFrame ‘df1’ with columns ‘A’, ‘B’, and ‘C’, writecode to calculate the sum of ‘A’ and ‘B’ and store the result in ‘C’.\n",
    "\n",
    "```\n",
    "'A': [1, 2, 3, 4, 5],\n",
    "'B': [6, 7, 8, 9, 10]\n",
    "```\n",
    "\n",
    "Note: This is an example of a non-element-wise operation in pandas. The ‘+’ operator is used to add the corresponding elements of ‘A’ and ‘B’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Now consider another DataFrame ‘df2’ with the same columns but different data. Modify your code to calculate the product of ‘A’ and ‘B’ and store the result in ‘C’.\n",
    "\n",
    "```\n",
    "'A': [1, 2, 3, 4, 5],\n",
    "'B': [6, 7, 8, 9, 10]\n",
    "```\n",
    "\n",
    "Note: The ‘*’ operator is used to multiply the corresponding elements of ‘A’ and ‘B’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Given a DataFrame ‘df3’ with columns ‘D’, ‘E’, and ‘F’, write code to calculate the sum of ‘D’ and ‘E’ using eval() function and store the result in ‘F’.\n",
    "\n",
    "```\n",
    "'D': [11, 12, 13, 14, 15],\n",
    "'E': [16, 17, 18, 19, 20]\n",
    "```\n",
    "\n",
    "Note: The eval() function in pandas is used to evaluate an expression that operates on columns in a DataFrame. It allows element-wise operations to be expressed more succinctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Modify your code to calculate the product of ‘D’ and ‘E’ using eval() function and store the result in ‘F’.\n",
    "\n",
    "Note: The eval() function allows for more complex expressions involving multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Given a DataFrame ‘df5’ with columns ‘Product’, ‘Sales_2019’, and ‘Sales_2020’, write code to filter rows where ‘Sales_2019’ is greater than ‘Sales_2020’ using query() function.\n",
    "\n",
    "```\n",
    "'Product': ['Product A', 'Product B', 'Product C', 'Product D', 'Product E'],\n",
    "'Sales_2019': [200, 300, 250, 350, 275],\n",
    "'Sales_2020': [220, 280, 260, 330, 290]\n",
    "```\n",
    "\n",
    "Note: The query() function in pandas is used to filter rows of a DataFrame based on a query expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Given a DataFrame ‘df6’ with columns ‘City’, ‘Population_2019’, and ‘Population_2020’, write code to first calculate the population growth from 2019 to 2020 using eval() function and store the result in a new column ‘Growth’. Then, use query() function to filter rows where ‘Growth’ is greater than 0.\n",
    "\n",
    "```\n",
    "'City': ['City F', 'City G', 'City H', 'City I', 'City J'],\n",
    "'Population_2019': [21000, 22000, 23000, 24000, 25000],\n",
    "'Population_2020': [21500, 22500, 22500, 23500, 24500]\n",
    "```\n",
    "\n",
    "Note: The eval() function in pandas is used to evaluate an expression that operates on columns in a DataFrame. It allows element-wise operations to be expressed more succinctly. The query() function is used to filter rows of a DataFrame based on a query expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. (Optional) Investigate implemented multi-indexing, eval() and query() in the same code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-indexing\n",
    "* Creating data frame structures with multiple levels of index hierarchy.\n",
    "* Useful for handling data with complex, multi-dimensional relationships.\n",
    "\n",
    "Benefits of multi-indexing:\n",
    "* A more flexible and expressive way to represent data with multiple dimensions.\n",
    "* Enables more efficient and convenient operations based on the index levels.\n",
    "    * Grouping,\n",
    "    * Sorting,\n",
    "    * Slicing,\n",
    "    * Aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Indexing\n",
    "data = {'Department': ['HR', 'HR', 'Engineering', 'Engineering'],\n",
    "        'Employee': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Salary': [60_000, 65_000, 80_000, 75_000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a hierarchical index\n",
    "hierarchical_df = df.set_index(['Department', 'Employee'])\n",
    "\n",
    "# Access HR department Bob data\n",
    "# using loc (labels) to access the data we want \n",
    "hierarchical_df.loc[('HR','Bob')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with employee data\n",
    "data = {'Department': ['HR', 'HR', 'Engineering', 'Engineering'],\n",
    "        'Employee': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Salary': [50000, 60000, 70000, 80000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Use eval() to calculate a new column 'Tax' \n",
    "# which is 10% of the salary\n",
    "df.eval('Tax = Salary * 0.1', inplace=True)\n",
    "\n",
    "# Use query() to filter employees \n",
    "# who are over 30 years old and have a tax greater than 5000\n",
    "filtered_df = df.query('Tax > 5000')\n",
    "\n",
    "# Create a hierarchical index on the filtered DataFrame\n",
    "hierarchical_df = filtered_df.set_index(['Department', 'Employee'])\n",
    "\n",
    "print(hierarchical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers \n",
    "\n",
    "These are values that deviate from the rest of the data. Handling outliers is essential for data preprocessing, as they can affect your analysis and decrease the relaibility of your models.  \n",
    "\n",
    "Identifying outliers  \n",
    "* One method is to use a boxplot. \n",
    "    * The box represents the interquartile range (IQR), which is hte range betwwen the 1st and 3rd quartiles of the data.\n",
    "    * The whiskers extend to show the range of data within 1.5 times the IQR fro the quartiles. \n",
    "    * Anthing outside this is a potential outlier.\n",
    "\n",
    "__NOTE__: Not everyting outside the plot is an outlier. However, they should be investigated.  \n",
    "* Outliers can tell us something important about potential errors in the data, or in the collection method.\n",
    "* It is important to understand the context and the domain when handling them.\n",
    "* Different methods for handling outliers include removing, replacing or transforming them. Which method we choose depends on the nature and purpose of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of random values\n",
    "data = {'Values': pd.Series(range(1,501)).sample(100, replace=True).tolist()}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display as a boxplot\n",
    "df.boxplot(column='Values')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for handling outliers  \n",
    "\n",
    "* NaN  \n",
    "    * Identify outliers and replace with NaN.\n",
    "    * Effectively removes them from calculations.\n",
    "* Clip Values\n",
    "    * Clipping sets upper and lower bounds on a variable.\n",
    "    * Limits extreme values to a specified range.\n",
    "    * Mitigates the impact of outliers without removing them entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN Outlier Handling\n",
    "# DataFrame representing sales data\n",
    "data = {'Sales': [200, 220, 250, 210, 3100, 230, 210, 2700, 240]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the average sales\n",
    "average_sales = df['Sales'].mean()\n",
    "\n",
    "# Define your threshold for outliers as significantly higher than the average\n",
    "threshold = average_sales * 1.5\n",
    "\n",
    "# Replace outliers with NaN\n",
    "# uncomment the next line to apply the threshold filter \n",
    "#df.loc[df['Sales'] > threshold, 'Sales'] = None  \n",
    "\n",
    "# Plot the sales data using a scatter plot\n",
    "df.reset_index().plot(kind='scatter', x='index', y='Sales', title='Sales Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clips value Outlier Handling\n",
    "# DataFrame representing student test scores\n",
    "data = {'Scores': [85, 90, 78, 92, 88, 76, 95, 89, 300, 84]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the scores data using a histogram plot\n",
    "df['Scores'].plot(kind='hist', title='Student Test Scores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define upper and lower bounds\n",
    "lower_bound = 0\n",
    "upper_bound = 100\n",
    "\n",
    "# Clip scores to the specified bounds\n",
    "df['Scores'] = df['Scores'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Plot the scores data using a histogram plot\n",
    "df['Scores'].plot(kind='hist', title='Student Test Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Given the DataFrame ‘df1’, identify outliers in the ‘Temperature’ column using a histogram.\n",
    "\n",
    "```\n",
    "df1 = pd.DataFrame({\n",
    "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "    'Temperature': [15, 16, 14, 18, 20, 22, 24, 23, 21, 19, 17, 15],\n",
    "    'Humidity': [30, 32, 35, 40, 45, 50, 55, 50, 45, 40, 35, 30],\n",
    "    'Rainfall': [50, 60, 80, 100, 120, 140, 160, 140, 120, 100, 80, 60],\n",
    "    'WindSpeed': [10, 11, 12, 13, 14, 15, 16, 15, 14, 13 ,12 ,11],\n",
    "    'Snowfall': [30 ,25 ,20 ,15 ,10 ,5 ,0 ,5 ,10 ,15 ,20 ,25],\n",
    "    'CloudCover': [6 ,7 ,8 ,9 ,10 ,9 ,8 ,7 ,6 ,5 ,4 ,3]\n",
    "})\n",
    "```\n",
    "\n",
    "Note: The plot() function with kind=‘hist’ is used to create a histogram. Outliers can be identified as values that are far from the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Now consider the same DataFrame ‘df1’. Modify your code to identify outliers in the ‘Humidity’ column using a box plot.\n",
    "\n",
    "Note: The plot() function with kind=‘box’ is used to create a box plot. Outliers can be identified as values that are outside the whiskers of the box plot. These are typically values that are more than 1.5 * IQR (Inter-Quartile Range) away from either the first quartile (25%) or third quartile (75%) in a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory optimisation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory optimisation  \n",
    "\n",
    "* Crucial when working with large datasets.  \n",
    "* Maintains data integrity while reducing memory consumption.\n",
    "\n",
    "Start with your data structure.\n",
    "* Correct data types  \n",
    "    * Use appropriate data types for columns.\n",
    "    * Consider using categorical data types for columns with limited unique values.\n",
    "* Sparse data structures\n",
    "    * Suitable for datasets with many missing values.\n",
    "    * Storing only non-missing values reduces memory usage.\n",
    "* Read data in chunks\n",
    "    * Read large datasets in smaller chunks.\n",
    "    * Avoids loading the entire dataset, so it is not all in memory at once.\n",
    "* Optimise `groupby` operations\n",
    "    * Avoid creating a new index, as this can consume additional memory.\n",
    "* Release unneeded dataframes\n",
    "    * Delete DataFrames or Series when no longer needed.\n",
    "    * Frees up memory for other operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data types  \n",
    "\n",
    "* Reduces memory usage by storing data in the most efficient format.\n",
    "* Improves performance by reducing the time and space complexity of operations\n",
    "* Maintains data integrity by ensuring that data is stored in a format that accurately represents its nature and domain\n",
    "* Makes the code more readable and maintainable by making explicit the type of data stored in each column\n",
    "\n",
    "__Analysing Sales Data__ example:\n",
    "* Use int8 or int16 for columns representing the number of items sold.\n",
    "* int8 and int16 data types are suitable for small value ranges.\n",
    "* Business will set what is needed in database and can convert back once analysis is complete.\n",
    "\n",
    "__Student grades__ example:\n",
    "* Use categorical data type for the column representing letter grades A, B, C, D, and F.\n",
    "* Suitable for columns with limited number of unique values.\n",
    "* Optimises memory usage and improves performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame representing sales data\n",
    "data = {'Items Sold': pd.Series([20, 30, 15, 25, 35, 40, 30, 25, 20], \n",
    "                                dtype='int16')}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the sales data using a line plot\n",
    "df.plot(kind='line', title='Number of Items Sold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame representing student grades\n",
    "data = {'Grades': pd.Series(['A', 'B', 'A', 'C', 'B', 'A', 'D', 'F', 'B'], \n",
    "                            dtype='category')}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the grades data using a bar plot\n",
    "df['Grades'].value_counts().sort_index().plot(kind='bar', title='Student Grades')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse data  \n",
    "\n",
    "* Time series data often has many missing values e.g. in sensor readings collected over time.\n",
    "* Sparse data structures are suitable for datasets with many missing values.\n",
    "    * Reduces memory usage.\n",
    "    * Storing only non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame representing sensor data with many missing values\n",
    "data = {'Sensor Data': pd.arrays.SparseArray([1.5, None, 2.0, \n",
    "                                              None, None, 3.5,\n",
    "                                                None, 4.0])}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add an index column to the DataFrame\n",
    "df['Index'] = df.index\n",
    "\n",
    "# Plot the sensor data using a scatter plot\n",
    "df.plot(kind='scatter', x='Index', \n",
    "        y='Sensor Data', title='Sensor Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Sparse Data Structures\n",
    "# DataFrame with many missing values\n",
    "df = pd.DataFrame({'A': [None, 2, None], 'B': [None, None, 3]})\n",
    "\n",
    "# Convert to sparse data structures\n",
    "df_sparse = df.astype(pd.SparseDtype(\"float\", pd.NA))\n",
    "\n",
    "# Plotting the memory usage\n",
    "df.memory_usage().plot(kind='bar', title='Memory usage before sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse.memory_usage().plot(kind='bar', title='Memory usage after sparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data in chunks  \n",
    "\n",
    "* Allows for processing large datasets that do not fit into memory.\n",
    "* Files and databases can be GB or even TB in size.\n",
    "* Improve performance by reducing memory usage, avoiding swapping between clusters, hard drives, system memory(RAM).\n",
    "* Code more scalable and robust to changes in the size of the input data.\n",
    "* Efficient computation on large datasets using streaming algorithms and incremental learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data in Chunks\n",
    "# Assuming we have a large CSV file \"large_dataset.csv\"\n",
    "# Created with Large Dataset CSV Creator.ipynb\n",
    "chunksize = 10 ** 5*5\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\"netflix-rotten-tomatoes-metacritic-imdb.csv\", chunksize=chunksize):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Combine chunks into one DataFrame\n",
    "df = pd.concat(chunks)\n",
    "\n",
    "# Plotting the memory usage\n",
    "df.memory_usage().plot(kind='bar', title='Memory usage (bytes) by column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release unneeded dataframes from memory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise GroupBy Operations\n",
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar'], \n",
    "                   'B': ['one', 'one', 'two', 'three'], \n",
    "                   'C': range(4), \n",
    "                   'D': range(4)})\n",
    "grouped = df.groupby(['A', 'B'], as_index=False).sum()\n",
    "\n",
    "# Plotting the result of GroupBy operations\n",
    "grouped.plot(x='A', y='C', kind='bar', title='Sum of C grouped by A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 - Investigate memory optimisations in Python and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. (Optional) Explore the following code and see how it works. Change values, try your own dataset, try larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the memory usage by changing the data types of the columns\n",
    "df1_optimized = df1.copy()\n",
    "df1_optimized['Temperature'] = df1_optimized['Temperature'].astype('float32')\n",
    "df1_optimized['Humidity'] = df1_optimized['Humidity'].astype('int32')\n",
    "\n",
    "# Check the memory usage of each column after optimisation\n",
    "print(df1_optimized.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the memory usage by changing the data types of the columns\n",
    "df1_optimized = df1.copy()\n",
    "df1_optimized['Temperature'] = df1_optimized['Temperature'].astype('float32')\n",
    "df1_optimized['Humidity'] = df1_optimized['Humidity'].astype('int32')\n",
    "\n",
    "# Check the memory usage of each column after optimisation\n",
    "print(df1_optimized.memory_usage(deep=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage before optimisation\n",
    "mem_before = df1.memory_usage(deep=True)\n",
    "\n",
    "# Memory usage after optimisation\n",
    "mem_after = df1_optimized.memory_usage(deep=True)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_mem = pd.DataFrame({'Before': mem_before, 'After': mem_after})\n",
    "\n",
    "# Plot the memory usage\n",
    "df_mem.plot(kind='bar', title='Memory Usage Before and After Optimisation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
