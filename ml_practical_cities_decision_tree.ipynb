{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Machine Learning practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the cities dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules we will need to use \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# suppressing some advisory warnings that don't affect what we're doing here\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# machine learning models and utilities \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset from GitHub\n",
    "data = \"https://raw.githubusercontent.com/jargonautical/r2d3-part-1-data/refs/heads/master/part_1_data.csv\"\n",
    "city_df = pd.read_csv(data)\n",
    "city_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our X and y  \n",
    "* X: the factors, attributes or features.  \n",
    "* y: the target variable or label.\n",
    "\n",
    "__NOTE__ upper case 'X' and lower case 'y' is a common naming convention in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = city_df.drop(columns = ['in_sf'], axis = 1)\n",
    "y = city_df['in_sf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split  \n",
    "\n",
    "* Once we have defined our features and our labels, we take a subset or 'slice' of the data to use in training our model.  \n",
    "* We can specify percentages to split on, or select a preset 'random state'.\n",
    "* Then we pass the `X` and `y` we have just defined to the function to create four new objects.\n",
    "    * `X_train` and `y_train` are the slices of `X` and `y` that will be used to train the model.\n",
    "    * `X_test` and `y_test` will be held back to test the model after it has been trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a model  \n",
    "\n",
    "* Now we need to call (or create, or instantiate) an instance of an object called `DecisionTreeClassifier`.\n",
    "* The steps are:\n",
    "    * Call an instance e.g. `clf = DecisionTreeClassifier()`\n",
    "    * Fit the instance `.fit()`\n",
    "    * And tell the model what data it is being fitted to `X_train, y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and evaluate  \n",
    "\n",
    "* Now we can call some other functions on our model using `clf.score()`.  \n",
    "* To see how well it managed to fit the training set, we call it on the training data `X_train, y_train`.\n",
    "* To see its performance on new and unseen data, we do the same but with `X_test, y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the model  \n",
    "* Most machine learning models in sklearn have specific methods for visualising them.\n",
    "* For a Decision Tree Classifier, we can call `tree.plot_tree()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "# Putting the feature names and class names into variables\n",
    "fn = X.columns\n",
    "cn = ['sf', 'ny']\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-efficients  \n",
    "* These are the values or *weights* the model has assigned to each of the features.\n",
    "* We can call and print them using `clf.feature_importances_`, and we can also plot them on a bar chart for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(clf, fn):\n",
    "    c_features = len(fn)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), fn)\n",
    "\n",
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf, fn)\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning  \n",
    "* Given our classifier's perfect score in training, and the drop in test score, it's possible our model is overfitted!\n",
    "* We can simplify and tune it in several ways.\n",
    "* The simplest option is to constrain it to a small number of levels using `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth = 4).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "# Putting the feature names and class names into variables\n",
    "fn = X.columns\n",
    "cn = ['sf', 'ny']\n",
    "tree.plot_tree(clf2,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);\n",
    "#fig.savefig('../images/plottreedefault.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf2, fn)\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf2.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection  \n",
    "* The goal is to find out which features have the most weight in the model (the largest coefficients).\n",
    "* If we can identify those, we can remove anything that has very little impact on the accuracy of the model overall.  \n",
    "* In this case, we can use `elevation` and `price_per_sqft` for a simpler model that is reasonably accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_df = city_df[['in_sf','elevation','price_per_sqft']]\n",
    "trim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trim_df.drop(columns = ['in_sf'], axis = 1)\n",
    "y = trim_df['in_sf']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "clf3 = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf3.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the feature names and class names into variables\n",
    "fn = X.columns\n",
    "cn = ['sf', 'ny']\n",
    "tree.plot_tree(clf3,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);\n",
    "#fig.savefig('../images/plottreedefault.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = X.columns\n",
    "\n",
    "def plot_feature_importances(clf, fn):\n",
    "    c_features = len(fn)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), fn)\n",
    "\n",
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf3, fn)\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
